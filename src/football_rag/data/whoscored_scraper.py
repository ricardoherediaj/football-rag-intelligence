"""
Simple WhoScored scraper based on working notebook code.
"""

import json
import time
import pandas as pd
import re
import argparse
from pathlib import Path
from typing import Optional, List
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By

from pydantic import BaseModel

import sys
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.football_rag.storage.minio_client import MinIOClient


class MatchEvent(BaseModel):
    id: int
    event_id: int
    minute: int
    second: Optional[float] = None
    team_id: int
    player_id: int
    x: float
    y: float
    end_x: Optional[float] = None
    end_y: Optional[float] = None
    qualifiers: List[dict]
    is_touch: bool
    blocked_x: Optional[float] = None
    blocked_y: Optional[float] = None
    goal_mouth_z: Optional[float] = None
    goal_mouth_y: Optional[float] = None
    is_shot: bool
    card_type: bool
    is_goal: bool
    type_display_name: str
    outcome_type_display_name: str
    period_display_name: str
    match_url: str


def scrape_single_match(url, driver):
    """Scrape match events from a single WhoScored URL"""
    print(f"Scraping: {url}")
    
    driver.get(url)
    time.sleep(3)
    
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    element = soup.select_one('script:-soup-contains("matchCentreData")')
    
    if not element:
        print("‚ùå matchCentreData not found")
        return None
    
    # Extract and parse match data
    match_data_raw = element.text.split("matchCentreData: ")[1].split(',\n')[0]
    matchdict = json.loads(match_data_raw)
    
    # Process events
    match_events = matchdict['events']
    df = pd.DataFrame(match_events)
    
    if df.empty:
        print("‚ö†Ô∏è No events found")
        return None
    
    # Clean and transform data
    df.dropna(subset='playerId', inplace=True)
    df = df.where(pd.notnull(df), None)
    
    # Rename columns
    df = df.rename({
        'eventId': 'event_id',
        'outcomeType': 'outcome_type',
        'isTouch': 'is_touch',
        'playerId': 'player_id',
        'teamId': 'team_id',
        'endX': 'end_x',
        'endY': 'end_y',
        'blockedX': 'blocked_x',
        'blockedY': 'blocked_y',
        'goalMouthZ': 'goal_mouth_z',
        'goalMouthY': 'goal_mouth_y',
        'isShot': 'is_shot',
        'cardType': 'card_type',
        'isGoal': 'is_goal'
    }, axis=1)
    
    # Add display names
    df['period_display_name'] = df['period'].apply(lambda x: x['displayName'])
    df['type_display_name'] = df['type'].apply(lambda x: x['displayName'])
    df['outcome_type_display_name'] = df['outcome_type'].apply(lambda x: x['displayName'])
    
    # Drop original columns
    df.drop(columns=["period", "type", "outcome_type"], inplace=True)
    
    # Handle missing columns
    if 'is_goal' not in df.columns:
        df['is_goal'] = False
    if 'card_type' not in df.columns:
        df['card_type'] = False
    
    # Filter and select columns
    df = df[~(df['type_display_name'] == "OffsideGiven")]
    
    df = df[[
        'id', 'event_id', 'minute', 'second', 'team_id', 'player_id', 'x', 'y', 'end_x', 'end_y',
        'qualifiers', 'is_touch', 'blocked_x', 'blocked_y', 'goal_mouth_z', 'goal_mouth_y', 'is_shot',
        'card_type', 'is_goal', 'type_display_name', 'outcome_type_display_name', 'period_display_name'
    ]]
    
    # Convert data types
    df[['id', 'event_id', 'minute', 'team_id', 'player_id']] = df[['id', 'event_id', 'minute', 'team_id', 'player_id']].astype('int64')
    df[['second', 'x', 'y', 'end_x', 'end_y']] = df[['second', 'x', 'y', 'end_x', 'end_y']].astype(float)
    df[['is_shot', 'is_goal', 'card_type']] = df[['is_shot', 'is_goal', 'card_type']].astype(bool)
    
    # Handle NaN values
    df['is_goal'] = df['is_goal'].fillna(False)
    df['is_shot'] = df['is_shot'].fillna(False)
    
    # Add match URL to each row
    df['match_url'] = url
    
    # Validate with Pydantic
    validated_events = []
    for _, row in df.iterrows():
        try:
            event = MatchEvent(**row.to_dict())
            validated_events.append(event.model_dump())
        except Exception as e:
            print(f"Validation error: {e}")
    
    validated_df = pd.DataFrame(validated_events)
    print(f'‚úÖ Successfully scraped {len(validated_df)} events')
    return validated_df


def extract_match_id(match_url: str) -> str:
    """Extract match ID from WhoScored URL.

    Args:
        match_url: Full WhoScored match URL

    Returns:
        Match ID as string
    """
    match = re.search(r'/matches/(\d+)/', match_url)
    if match:
        return match.group(1)
    raise ValueError(f"Could not extract match ID from URL: {match_url}")


def collect_all_season_matches(driver, exclude_match_ids: Optional[set] = None):
    """Navigate through calendar to collect all season matches.

    Args:
        driver: Selenium WebDriver instance
        exclude_match_ids: Set of match IDs to exclude (already scraped)

    Returns:
        List of match URLs to scrape
    """
    fixtures_url = "https://www.whoscored.com/Regions/155/Tournaments/13/Seasons/10752/Stages/24542/Fixtures/Netherlands-Eredivisie-2025-2026"
    driver.get(fixtures_url)
    time.sleep(5)

    all_match_urls = set()
    exclude_match_ids = exclude_match_ids or set()

    # Go back to August
    for _ in range(15):
        try:
            prev_button = driver.find_element(By.ID, "dayChangeBtn-prev")
            prev_button.click()
            time.sleep(2)
        except Exception:
            break

    # Go forward collecting all finished matches
    for _ in range(30):
        soup = BeautifulSoup(driver.page_source, 'html.parser')

        containers = soup.find_all('div', class_='Match-module_match__XlKTY')
        for container in containers:
            if container.find('span', class_='Match-module_FT__2rmH7'):
                score_link = container.find('a', class_='Match-module_score__5Ghhj')
                if score_link and score_link.get('href'):
                    match_url = f"https://www.whoscored.com{score_link['href']}"
                    match_id = extract_match_id(match_url)

                    if match_id not in exclude_match_ids:
                        all_match_urls.add(match_url)

        try:
            next_button = driver.find_element(By.ID, "dayChangeBtn-next")
            next_button.click()
            time.sleep(2)
        except Exception:
            break

    return list(all_match_urls)


def collect_and_scrape_complete_season(driver, exclude_match_ids: Optional[set] = None):
    """Complete season collection with detailed progress output.

    Args:
        driver: Selenium WebDriver instance
        exclude_match_ids: Set of match IDs to exclude (already scraped)
    """

    print("Step 1: Collecting all match URLs...")
    all_match_urls = collect_all_season_matches(driver, exclude_match_ids)
    print(f"Found {len(all_match_urls)} finished matches\n")
    
    if not all_match_urls:
        print("No matches found. Exiting.")
        return None
    
    print("Step 2: Scraping match data...")
    all_match_data = []
    
    for i, url in enumerate(all_match_urls, 1):
        print(f"Scraping match {i}/{len(all_match_urls)}")
        print(f"Scraping: {url}")
        
        try:
            match_df = scrape_single_match(url, driver)
            
            if match_df is not None:
                all_match_data.append(match_df)
                print(f"‚úÖ Successfully scraped {len(match_df)} events\n")
            else:
                print("‚ùå No data returned\n")
                
        except Exception as e:
            print(f"‚ùå Error scraping {url}: {str(e)}\n")
            continue
        
        time.sleep(2)
    
    if all_match_data:
        combined_df = pd.concat(all_match_data, ignore_index=True)
        print(f"‚úÖ Successfully scraped {len(all_match_data)} matches with {len(combined_df)} total events")
        return combined_df
    else:
        print("‚ùå No match data was scraped")
        return None


def scrape_complete_season(mode: str = "full", league: str = "eredivisie", season: str = "2025-2026"):
    """Main function to scrape complete season.

    Args:
        mode: 'full' for complete scrape, 'incremental' for only new matches
        league: League name (default: eredivisie)
        season: Season identifier (default: 2025-2026)

    Returns:
        DataFrame with scraped match data
    """
    driver = webdriver.Chrome()

    try:
        exclude_match_ids = set()

        if mode == "incremental":
            print("üîÑ Incremental mode: Checking for already scraped matches...")
            minio_client = MinIOClient()
            prefix = f"whoscored/{league}/{season}/"
            exclude_match_ids = minio_client.get_scraped_match_ids(prefix)
            print(f"Found {len(exclude_match_ids)} already scraped matches\n")

        print(f"Starting {mode} season collection...\n")
        season_data = collect_and_scrape_complete_season(driver, exclude_match_ids)
        return season_data

    finally:
        driver.quit()


def save_matches_to_minio(matches_df: pd.DataFrame, league: str = "eredivisie", season: str = "2025-2026") -> int:
    """Save individual matches to MinIO as JSON files.

    Args:
        matches_df: DataFrame containing all match events
        league: League name (default: eredivisie)
        season: Season identifier (default: 2025-2026)

    Returns:
        Number of matches uploaded successfully
    """
    minio_client = MinIOClient()
    prefix = f"whoscored/{league}/{season}/"

    uploaded_count = 0

    for match_url in matches_df['match_url'].unique():
        match_id = extract_match_id(match_url)

        if minio_client.match_exists(prefix, match_id):
            print(f"‚è≠Ô∏è  Match {match_id} already exists in MinIO, skipping...")
            continue

        match_events = matches_df[matches_df['match_url'] == match_url].to_dict(orient='records')

        match_data = {
            'match_id': match_id,
            'match_url': match_url,
            'league': league,
            'season': season,
            'events': match_events
        }

        object_name = f"{prefix}match_{match_id}.json"

        try:
            minio_client.upload_json(object_name, match_data)
            print(f"‚úÖ Uploaded match {match_id} to MinIO ({len(match_events)} events)")
            uploaded_count += 1
        except Exception as e:
            print(f"‚ùå Failed to upload match {match_id}: {e}")

    return uploaded_count


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="WhoScored scraper for Eredivisie matches")
    parser.add_argument(
        "--mode",
        choices=["full", "incremental"],
        default="incremental",
        help="Scraping mode: 'full' for complete scrape, 'incremental' for only new matches"
    )
    parser.add_argument("--league", default="eredivisie", help="League name")
    parser.add_argument("--season", default="2025-2026", help="Season identifier")

    args = parser.parse_args()

    season_data = scrape_complete_season(mode=args.mode, league=args.league, season=args.season)

    if season_data is not None and len(season_data) > 0:
        # Ensure data directory exists
        Path("data/raw").mkdir(parents=True, exist_ok=True)

        # Save to local CSV (backup)
        local_file = f'data/raw/{args.league}_{args.season.replace("-", "_")}_whoscored.csv'
        season_data.to_csv(local_file, index=False)
        print(f"‚úÖ Saved locally: {local_file}")

        # Save to MinIO
        uploaded = save_matches_to_minio(season_data, league=args.league, season=args.season)

        print("\nüìä Final Results:")
        print(f"- Total events scraped: {len(season_data)}")
        print(f"- Unique matches scraped: {season_data['match_url'].nunique()}")
        print(f"- Matches uploaded to MinIO: {uploaded}")
    else:
        print("\n‚ö†Ô∏è  No new matches to scrape")