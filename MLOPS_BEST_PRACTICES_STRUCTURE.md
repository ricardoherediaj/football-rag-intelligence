# Football RAG Intelligence - MLOps/LLMOps Best Practices Structure

## ðŸŽ¯ Philosophy: **Educational Best Practices**
- âœ… Real MLOps patterns (not toys)
- âœ… Clean, understandable code
- âœ… Industry-standard tools (Prefect, MLflow, Opik)
- âœ… Proper separation of concerns
- âŒ Over-engineering
- âŒ Unnecessary abstractions

## ðŸ“ Clean MLOps Structure

```
football-rag-intelligence/
â”œâ”€â”€ src/football_rag/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ settings.py              # Pydantic Settings (best practice)
â”‚   â”‚   â””â”€â”€ logging_config.py        # Structured logging setup
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ schemas.py               # Pydantic models (data contracts)
â”‚   â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py              # Abstract scraper pattern
â”‚   â”‚   â”‚   â”œâ”€â”€ whoscored.py         # Concrete implementation
â”‚   â”‚   â”‚   â””â”€â”€ fotmob.py            # Concrete implementation
â”‚   â”‚   â”œâ”€â”€ preprocessing.py         # Data transformation
â”‚   â”‚   â””â”€â”€ storage.py               # Data persistence layer
â”‚   â”œâ”€â”€ flows/                       # Prefect workflows (orchestration)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py        # ETL flow
â”‚   â”‚   â”œâ”€â”€ model_training.py        # Embedding/model updates
â”‚   â”‚   â””â”€â”€ inference.py             # RAG pipeline flow
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ embeddings.py            # Embedding model wrapper
â”‚   â”‚   â”œâ”€â”€ llm.py                   # LLM wrapper (Llama/OpenAI)
â”‚   â”‚   â””â”€â”€ rag_pipeline.py          # RAG orchestration
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ metrics.py               # Custom evaluation metrics
â”‚   â”‚   â”œâ”€â”€ evaluators.py            # LLM-as-judge implementations
â”‚   â”‚   â””â”€â”€ experiments.py           # MLflow experiment management
â”‚   â”œâ”€â”€ monitoring/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ opik_tracer.py           # LLM call tracing
â”‚   â”‚   â”œâ”€â”€ mlflow_logger.py         # Experiment logging
â”‚   â”‚   â””â”€â”€ alerts.py                # Quality alerts
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py                  # FastAPI app
â”‚   â”‚   â”œâ”€â”€ endpoints.py             # API routes
â”‚   â”‚   â””â”€â”€ dependencies.py          # Dependency injection
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ database.py              # DB connections
â”‚       â””â”€â”€ visualization.py         # Chart generation
â”œâ”€â”€ flows/                           # Prefect deployment files
â”‚   â”œâ”€â”€ deployments.py               # Flow deployments
â”‚   â””â”€â”€ schedules.py                 # Scheduling configuration
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ settings.yaml                # Environment configuration
â”‚   â”œâ”€â”€ model_config.yaml           # Model hyperparameters
â”‚   â””â”€â”€ evaluation_config.yaml      # Evaluation settings
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_infrastructure.py     # Initialize services
â”‚   â”œâ”€â”€ run_evaluation.py           # Manual evaluation runs
â”‚   â””â”€â”€ deploy.py                    # Deployment utilities
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py                  # Pytest fixtures
â”‚   â”œâ”€â”€ unit/                       # Unit tests
â”‚   â”œâ”€â”€ integration/                # Integration tests
â”‚   â””â”€â”€ e2e/                        # End-to-end tests
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile                  # Application container
â”‚   â”œâ”€â”€ docker-compose.yml          # Development services
â”‚   â””â”€â”€ docker-compose.prod.yml     # Production services
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ test.yml                # CI pipeline
â”‚       â”œâ”€â”€ deploy.yml              # CD pipeline
â”‚       â””â”€â”€ quality.yml             # Code quality checks
â”œâ”€â”€ requirements/
â”‚   â”œâ”€â”€ base.txt                    # Core dependencies
â”‚   â”œâ”€â”€ dev.txt                     # Development tools
â”‚   â””â”€â”€ prod.txt                    # Production extras
â”œâ”€â”€ pyproject.toml                  # Modern Python packaging
â”œâ”€â”€ prefect.yaml                    # Prefect configuration
â”œâ”€â”€ mlflow.yaml                     # MLflow configuration
â””â”€â”€ README.md                       # Clear documentation
```

## ðŸ”§ Best Practices Implementation

### 1. **Configuration Management (Pydantic Settings)**
```python
# src/football_rag/config/settings.py
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    # Database
    minio_endpoint: str = "localhost:9000"
    chroma_host: str = "localhost"
    chroma_port: int = 8000
    
    # Models  
    embedding_model: str = "all-mpnet-base-v2"
    llm_model: str = "llama3.2:1b"
    
    # MLOps
    mlflow_tracking_uri: str = "http://localhost:5000"
    opik_api_key: str = ""
    
    class Config:
        env_file = ".env"
        case_sensitive = False
```

### 2. **Orchestration with Prefect (Clean Flows)**
```python
# src/football_rag/flows/data_ingestion.py
from prefect import flow, task
from prefect.logging import get_run_logger

@task(retries=3, retry_delay_seconds=60)
async def scrape_match_data(match_id: str) -> dict:
    logger = get_run_logger()
    logger.info(f"Scraping match {match_id}")
    # Scraping logic
    return match_data

@task
async def process_and_store(match_data: dict) -> str:
    # Processing logic
    return storage_path

@flow(name="football-data-ingestion")
async def ingest_match_data(match_ids: list[str]):
    """Main data ingestion flow with proper error handling."""
    results = []
    for match_id in match_ids:
        match_data = await scrape_match_data(match_id)
        storage_path = await process_and_store(match_data)
        results.append(storage_path)
    return results
```

### 3. **Experiment Tracking (MLflow)**
```python
# src/football_rag/evaluation/experiments.py
import mlflow
from mlflow.models import infer_signature

class ExperimentManager:
    def __init__(self):
        mlflow.set_tracking_uri("http://localhost:5000")
        
    def log_embedding_experiment(self, model_name: str, metrics: dict):
        with mlflow.start_run(experiment_id="embedding-experiments"):
            mlflow.log_param("model_name", model_name)
            mlflow.log_metrics(metrics)
            mlflow.log_model(model, "embedding_model")
```

### 4. **LLM Monitoring (Opik)**
```python
# src/football_rag/monitoring/opik_tracer.py
from opik import track

@track
def rag_query(question: str, context: list[str]) -> str:
    """Tracked RAG query for monitoring."""
    response = llm.generate(question, context)
    return response

# Automatic tracking of:
# - Input/output
# - Latency
# - Token usage  
# - Quality scores
```

### 5. **Data Contracts (Pydantic Schemas)**
```python
# src/football_rag/data/schemas.py
from pydantic import BaseModel, Field
from datetime import datetime

class MatchEvent(BaseModel):
    match_id: str = Field(..., description="Unique match identifier")
    event_type: str = Field(..., description="Type of event")
    timestamp: datetime = Field(..., description="When event occurred")
    player_id: str | None = Field(None, description="Player involved")
    
    class Config:
        json_schema_extra = {
            "example": {
                "match_id": "123456",
                "event_type": "shot",
                "timestamp": "2024-01-01T15:30:00Z",
                "player_id": "player_123"
            }
        }
```

### 6. **Evaluation Framework**
```python
# src/football_rag/evaluation/evaluators.py
from mlflow.evaluate import make_eval_function

@make_eval_function
def faithfulness_evaluator(prediction: str, context: str) -> dict:
    """LLM-as-judge for faithfulness evaluation."""
    score = llm_judge.evaluate_faithfulness(prediction, context)
    return {"faithfulness_score": score}

# Automatic evaluation on each model update
```

## ðŸš€ Development Workflow

### Phase 1: Foundation (Week 1-2)
1. **Setup infrastructure**: Docker Compose with all services
2. **Basic data pipeline**: Scrapers â†’ Storage â†’ Processing
3. **Simple RAG**: Embeddings â†’ Retrieval â†’ Generation
4. **Monitoring setup**: MLflow + Opik integration

### Phase 2: Best Practices (Week 3-4)  
1. **Prefect flows**: Convert scripts to orchestrated workflows
2. **Evaluation framework**: Automated quality assessment
3. **Experiment tracking**: A/B test prompts and models
4. **API development**: Production-ready FastAPI

### Phase 3: Production (Week 5-6)
1. **CI/CD pipeline**: GitHub Actions for quality gates
2. **Deployment**: HuggingFace Spaces with monitoring
3. **Performance optimization**: Caching, async, scaling
4. **Documentation**: Complete setup and usage guides

## ðŸ’¡ Key Learning Outcomes

You'll learn:
- **Prefect**: Modern workflow orchestration
- **MLflow**: Experiment tracking and model registry
- **Opik**: LLM observability and monitoring  
- **Pydantic**: Data validation and settings management
- **FastAPI**: Production API development
- **Docker**: Containerization and service orchestration
- **Testing**: Unit, integration, and E2E testing
- **CI/CD**: Automated quality gates and deployment

This structure teaches you **real MLOps practices** without overwhelming complexity!